{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if processing is needed for: Labianca et al._2001_OrgSci_Emulation in Academia_Quant.pdf\n",
      "Processing Labianca et al._2001_OrgSci_Emulation in Academia_Quant.pdf: no existing record found.\n",
      "Gathering: Labianca et al._2001_OrgSci_Emulation in Academia_Quant.pdf\n",
      "Formatted search query: Labianca et al. 2001 Emulation in Academia Quant\n",
      "Raw div content: G Labianca, JF Fairbank, JB Thomas… - Organization …, 2001 - pubsonline.informs.org\n",
      "Extracted authors: G Labianca, JF Fairbank, JB Thomas…\n",
      "Extracted journal: Organization …\n",
      "Extracted year: 2001\n",
      "Sleeping for 24.87 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import urllib.parse\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def format_search_query(input_string):\n",
    "    # Remove the '.pdf' extension if present\n",
    "    if input_string.lower().endswith('.pdf'):\n",
    "        input_string = input_string[:-4]\n",
    "    \n",
    "    # Split the string by underscores\n",
    "    parts = input_string.split(\"_\")\n",
    "    \n",
    "    # Rebuild the string excluding the part between the second and third underscore\n",
    "    if len(parts) > 3:\n",
    "        input_string = '_'.join(parts[:2] + parts[3:])\n",
    "    else:\n",
    "        input_string = '_'.join(parts)  # If there aren't enough parts, just rejoin what is there\n",
    "\n",
    "    # Replace underscores with spaces and fix HTML character entities\n",
    "    input_string = input_string.replace(\"_\", \" \")\n",
    "    input_string = input_string.replace(\"&amp\", \"&\")\n",
    "    input_string = input_string.replace(\";\", \"\")\n",
    "\n",
    "    # Print the intermediate result to check\n",
    "    print(\"Formatted search query:\", input_string)\n",
    "\n",
    "    # URL encode the formatted string\n",
    "    formatted_string = urllib.parse.quote_plus(input_string)\n",
    "    return formatted_string\n",
    "\n",
    "\n",
    "def extract_paper_info(filename):\n",
    "    formatted_query = format_search_query(filename)\n",
    "    scholar_url = f\"https://scholar.google.com/scholar?hl=en&q={formatted_query}&as_sdt=0,5\"\n",
    "    driver.get(scholar_url)\n",
    "    try:\n",
    "        # Wait for the page element to load. If not loaded within 5 seconds, assume CAPTCHA has appeared\n",
    "        element_present = WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.ID, \"gs_res_ccl_mid\"))\n",
    "        )\n",
    "    except:\n",
    "        # If element is not found, it could be due to CAPTCHA\n",
    "        print(\"CAPTCHA REACHED\")\n",
    "        return None\n",
    "    try:\n",
    "        # WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, \"gs_res_ccl_mid\")))\n",
    "        result = driver.find_element(By.CSS_SELECTOR, '.gs_ri')\n",
    "        title = result.find_element(By.CSS_SELECTOR, 'h3.gs_rt a').text\n",
    "        \n",
    "        try:\n",
    "            authors_and_journal_div = result.find_element(By.CSS_SELECTOR, 'div.gs_a.gs_fma_p').text\n",
    "        except Exception:\n",
    "            # Fallback selector\n",
    "            authors_and_journal_div = result.find_element(By.CSS_SELECTOR, 'div.gs_a').text\n",
    "\n",
    "        print(f\"Raw div content: {authors_and_journal_div}\")\n",
    "\n",
    "        # Split the div content appropriately\n",
    "        parts = authors_and_journal_div.split(' - ')\n",
    "        if len(parts) >= 2:\n",
    "            authors = parts[0].strip()\n",
    "            journal_info = parts[1].split(', ')\n",
    "            journal_name = journal_info[0].strip()\n",
    "            year = journal_info[1].split(' ')[0].strip() if len(journal_info) > 1 else None\n",
    "        elif len(parts) == 1:\n",
    "            parts = authors_and_journal_div.split('\\n')\n",
    "            if len(parts) > 1:\n",
    "                # If no dash was found, process as comma-separated format\n",
    "                authors = parts[0].strip()\n",
    "                remaining_info = parts[1]\n",
    "\n",
    "                # Use regex to find the year\n",
    "                year_match = re.search(r'\\b(19|20)\\d{2}\\b', remaining_info)\n",
    "                if year_match:\n",
    "                    year = year_match.group()\n",
    "                    \n",
    "                    # Extract journal name by taking everything between the authors and year\n",
    "                    journal_start_idx = 0\n",
    "                    journal_end_idx = year_match.start()\n",
    "                    journal_name = remaining_info[journal_start_idx:journal_end_idx].strip(', ')\n",
    "                else:\n",
    "                    year = None\n",
    "                    journal_name = None\n",
    "            else: \n",
    "                authors = None\n",
    "                journal_name = None\n",
    "                year = None\n",
    "        else:\n",
    "            # If parsing fails, set to None or default values\n",
    "            authors = None\n",
    "            journal_name = None\n",
    "            year = None\n",
    "\n",
    "        print(f\"Extracted authors: {authors}\")\n",
    "        print(f\"Extracted journal: {journal_name}\")\n",
    "        print(f\"Extracted year: {year}\")\n",
    "\n",
    "        try:\n",
    "            cited_by_text = result.find_element(By.PARTIAL_LINK_TEXT, 'Cited by').text\n",
    "            cited_by = cited_by_text.split(' ')[2]\n",
    "        except Exception:\n",
    "            cited_by = None\n",
    "            print(\"No 'Cited by' info available.\")\n",
    "\n",
    "        year_in_filename = year in filename if year else False\n",
    "\n",
    "        sleep_duration = random.uniform(10, 30)\n",
    "        print(f\"Sleeping for {sleep_duration:.2f} seconds\")\n",
    "        time.sleep(sleep_duration)\n",
    "\n",
    "        return [title, authors, journal_name, year, cited_by, filename, year_in_filename]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        sleep_duration = random.uniform(10, 30)\n",
    "        print(f\"Sleeping for {sleep_duration:.2f} seconds due to error\")\n",
    "        time.sleep(sleep_duration)\n",
    "        return [None, None, None, None, None, filename, False]\n",
    "\n",
    "\n",
    "def load_existing_entries(year):\n",
    "    try:\n",
    "        df = pd.read_excel(f'../paper_citation_counts/{year}_paper_citation_info.xlsx')\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame(columns=['Title', 'Authors', 'Journal', 'Year', 'Cited By', 'File Name', 'Year in Filename'])\n",
    "\n",
    "\n",
    "def should_process_file(df, filename):\n",
    "    print(f\"Checking if processing is needed for: {filename}\")\n",
    "    existing_entry = df[df['File Name'].str.strip().str.lower() == filename.strip().lower()]\n",
    "    \n",
    "    if not existing_entry.empty:\n",
    "        row = existing_entry.iloc[0]\n",
    "        # Check all required fields for completeness\n",
    "        if pd.notna(row['Title']) and pd.notna(row['Authors']) and pd.notna(row['Journal']) and pd.notna(row['Cited By']):\n",
    "            print(f\"Skipping {filename}: complete record exists.\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"Processing {filename}: incomplete record.\")\n",
    "            return True\n",
    "    else:\n",
    "        print(f\"Processing {filename}: no existing record found.\")\n",
    "        return True\n",
    "\n",
    "# Setup Selenium WebDriver\n",
    "options = Options()\n",
    "# options.add_argument(\"--headless\")\n",
    "service = Service('/usr/local/bin/chromedriver')\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "try:\n",
    "    year = 2001\n",
    "    directory = f'../papers_by_year/{year}'\n",
    "    filenames = [f for f in os.listdir(directory) if f.endswith('.pdf')]\n",
    "    \n",
    "    # Load existing entries\n",
    "    existing_entries = load_existing_entries(year)\n",
    "    \n",
    "    # DataFrame to store paper info from this processing batch\n",
    "    data = []\n",
    "\n",
    "    # Process each file\n",
    "    for filename in filenames:\n",
    "        if should_process_file(existing_entries, filename):\n",
    "            print(\"Gathering:\", filename)\n",
    "            info = extract_paper_info(filename)\n",
    "            if info is not None:\n",
    "                print(info)\n",
    "                data.append(info)\n",
    "            else:\n",
    "                print(\"CAPTCHA or other issue encountered, stopping further processing.\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Skipping file {filename} as it already has complete information.\")\n",
    "\n",
    "    if data:  # Check if any new data was collected\n",
    "        # Convert new data to DataFrame\n",
    "        new_df = pd.DataFrame(data, columns=['Title', 'Authors', 'Journal', 'Year', 'Cited By', 'File Name', 'Year in Filename'])\n",
    "        \n",
    "        # Combine with existing entries\n",
    "        if not existing_entries.empty:\n",
    "            combined_df = pd.concat([existing_entries, new_df], ignore_index=True)\n",
    "        else:\n",
    "            combined_df = new_df\n",
    "\n",
    "        # Remove duplicates, in case some files are processed again\n",
    "        combined_df.drop_duplicates(subset=['File Name'], keep='last', inplace=True)\n",
    "\n",
    "        # Save combined DataFrame to Excel, replacing the old file\n",
    "        combined_df.to_excel(f'../paper_citation_counts/{year}_paper_citation_info.xlsx', index=False)\n",
    "    else:\n",
    "        print(\"No data collected; no new file saved.\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
